{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9997b921-ecdc-45d2-b82e-f73cd3281893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4176ac-8bcd-4b0f-a7a3-2489af1403b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Package Imports\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039d6bf0-4720-440a-9762-7c0a01b5620d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enter Databricks account hash here:\n",
    "db_account_hash = ''\n",
    "\n",
    "# Enter Databricks Provider Name here:\n",
    "provider_name = ''\n",
    "\n",
    "# Enter Contact Email here:\n",
    "business_contact_email = ''\n",
    "\n",
    "# Enter TOS Link here:\n",
    "terms_of_service_link = ''\n",
    "\n",
    "# Enter Privacy Policy Link here:\n",
    "privacy_policy_link = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8870ae2a-fd54-4b27-bce6-82cf86fc4b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the API URL associated with the notebook's context, if it exists\n",
    "host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "\n",
    "# Retrieve the API token associated with the notebook's context, if it exists\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "# Retrieve the user associated with the notebook's context, if it exists\n",
    "user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").getOrElse(None)\n",
    "\n",
    "# Retrieve the workspace ID associated with the notebook's context, if it exists\n",
    "workspace_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"orgId\").getOrElse(None)\n",
    "\n",
    "# Convert the notebook's context to a dictionary\n",
    "ctx = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())\n",
    "\n",
    "# Extract the API URL from the 'extraContext' key of the notebook's context dictionary\n",
    "host_name = ctx['extraContext']['api_url']\n",
    "\n",
    "# Extract the API token from the 'extraContext' key of the notebook's context dictionary\n",
    "host_token = ctx['extraContext']['api_token']\n",
    "\n",
    "# Extract the notebook path from the 'extraContext' key of the notebook's context dictionary\n",
    "notebook_path = ctx['extraContext']['notebook_path']\n",
    "\n",
    "# Set Deployment Name:\n",
    "deployment_name = db_account_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cda969-1f09-4385-817d-13dd5e6d7cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mInvalidURL\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-3716794575589390>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## Get all Marketplace listings\u001B[39;00m\n\u001B[1;32m      2\u001B[0m get_listings_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdb_account_hash\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.cloud.databricks.com/api/2.0/marketplace-provider/listings\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m      4\u001B[0m           get_listings_url,\n\u001B[1;32m      5\u001B[0m           headers\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAuthorization\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBearer \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m token})\n\u001B[1;32m      7\u001B[0m r_json \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m      8\u001B[0m r_json \u001B[38;5;241m=\u001B[39m [listing \u001B[38;5;28;01mfor\u001B[39;00m listing \u001B[38;5;129;01min\u001B[39;00m r_json[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlistings\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/api.py:75\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/api.py:61\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/sessions.py:515\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;66;03m# Create the Request.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m req \u001B[38;5;241m=\u001B[39m Request(\n\u001B[1;32m    504\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod\u001B[38;5;241m.\u001B[39mupper(),\n\u001B[1;32m    505\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    513\u001B[0m     hooks\u001B[38;5;241m=\u001B[39mhooks,\n\u001B[1;32m    514\u001B[0m )\n\u001B[0;32m--> 515\u001B[0m prep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m proxies \u001B[38;5;241m=\u001B[39m proxies \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    519\u001B[0m settings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerge_environment_settings(\n\u001B[1;32m    520\u001B[0m     prep\u001B[38;5;241m.\u001B[39murl, proxies, stream, verify, cert\n\u001B[1;32m    521\u001B[0m )\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/sessions.py:443\u001B[0m, in \u001B[0;36mSession.prepare_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    440\u001B[0m     auth \u001B[38;5;241m=\u001B[39m get_netrc_auth(request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    442\u001B[0m p \u001B[38;5;241m=\u001B[39m PreparedRequest()\n\u001B[0;32m--> 443\u001B[0m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    448\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    449\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdict_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCaseInsensitiveDict\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerged_cookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m p\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/models.py:318\u001B[0m, in \u001B[0;36mPreparedRequest.prepare\u001B[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_method(method)\n\u001B[0;32m--> 318\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_headers(headers)\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_cookies(cookies)\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/models.py:407\u001B[0m, in \u001B[0;36mPreparedRequest.prepare_url\u001B[0;34m(self, url, params)\u001B[0m\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidURL(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mURL has an invalid label.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m host\u001B[38;5;241m.\u001B[39mstartswith((\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m--> 407\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidURL(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mURL has an invalid label.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    409\u001B[0m \u001B[38;5;66;03m# Carefully reconstruct the network location\u001B[39;00m\n\u001B[1;32m    410\u001B[0m netloc \u001B[38;5;241m=\u001B[39m auth \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n\n\u001B[0;31mInvalidURL\u001B[0m: URL has an invalid label."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mInvalidURL\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-3716794575589390>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## Get all Marketplace listings\u001B[39;00m\n\u001B[1;32m      2\u001B[0m get_listings_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdb_account_hash\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.cloud.databricks.com/api/2.0/marketplace-provider/listings\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m      4\u001B[0m           get_listings_url,\n\u001B[1;32m      5\u001B[0m           headers\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAuthorization\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBearer \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m token})\n\u001B[1;32m      7\u001B[0m r_json \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m      8\u001B[0m r_json \u001B[38;5;241m=\u001B[39m [listing \u001B[38;5;28;01mfor\u001B[39;00m listing \u001B[38;5;129;01min\u001B[39;00m r_json[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlistings\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/api.py:75\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/api.py:61\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/sessions.py:515\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;66;03m# Create the Request.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m req \u001B[38;5;241m=\u001B[39m Request(\n\u001B[1;32m    504\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod\u001B[38;5;241m.\u001B[39mupper(),\n\u001B[1;32m    505\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    513\u001B[0m     hooks\u001B[38;5;241m=\u001B[39mhooks,\n\u001B[1;32m    514\u001B[0m )\n\u001B[0;32m--> 515\u001B[0m prep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m proxies \u001B[38;5;241m=\u001B[39m proxies \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    519\u001B[0m settings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerge_environment_settings(\n\u001B[1;32m    520\u001B[0m     prep\u001B[38;5;241m.\u001B[39murl, proxies, stream, verify, cert\n\u001B[1;32m    521\u001B[0m )\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/sessions.py:443\u001B[0m, in \u001B[0;36mSession.prepare_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    440\u001B[0m     auth \u001B[38;5;241m=\u001B[39m get_netrc_auth(request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    442\u001B[0m p \u001B[38;5;241m=\u001B[39m PreparedRequest()\n\u001B[0;32m--> 443\u001B[0m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    448\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    449\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdict_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCaseInsensitiveDict\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerged_cookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m p\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/models.py:318\u001B[0m, in \u001B[0;36mPreparedRequest.prepare\u001B[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_method(method)\n\u001B[0;32m--> 318\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_headers(headers)\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_cookies(cookies)\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/requests/models.py:407\u001B[0m, in \u001B[0;36mPreparedRequest.prepare_url\u001B[0;34m(self, url, params)\u001B[0m\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidURL(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mURL has an invalid label.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m host\u001B[38;5;241m.\u001B[39mstartswith((\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m--> 407\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidURL(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mURL has an invalid label.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    409\u001B[0m \u001B[38;5;66;03m# Carefully reconstruct the network location\u001B[39;00m\n\u001B[1;32m    410\u001B[0m netloc \u001B[38;5;241m=\u001B[39m auth \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n\n\u001B[0;31mInvalidURL\u001B[0m: URL has an invalid label.",
       "errorSummary": "<span class='ansi-red-fg'>InvalidURL</span>: URL has an invalid label.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Get all Marketplace listings\n",
    "get_listings_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/marketplace-provider/listings\"\n",
    "response = requests.get(\n",
    "          get_listings_url,\n",
    "          headers={\"Authorization\": \"Bearer \" + token})\n",
    "\n",
    "r_json = response.json()\n",
    "r_json = [listing for listing in r_json['listings']]\n",
    "\n",
    "print(f\"API returned {len(response.json()['listings'])} listing(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64314ee-4e0c-4dc7-b94a-b110f9a3084f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2559118402200095>:9\u001B[0m\n\u001B[1;32m      5\u001B[0m   num_tables \u001B[38;5;241m=\u001B[39m df_tables\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m      7\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m num_tables\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m listing \u001B[38;5;129;01min\u001B[39;00m r_json:\n\u001B[1;32m     10\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m     listing[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshare\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_tables\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m get_num_tables_in_share(listing[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshare\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\n\u001B[0;31mNameError\u001B[0m: name 'r_json' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2559118402200095>:9\u001B[0m\n\u001B[1;32m      5\u001B[0m   num_tables \u001B[38;5;241m=\u001B[39m df_tables\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m      7\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m num_tables\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m listing \u001B[38;5;129;01min\u001B[39;00m r_json:\n\u001B[1;32m     10\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m     listing[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshare\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_tables\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m get_num_tables_in_share(listing[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshare\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\n\u001B[0;31mNameError\u001B[0m: name 'r_json' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'r_json' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## For each share, find number of tables in share and add this number to this listing metadata to determine what type of Pandas Profiling to use\n",
    "def get_num_tables_in_share(share_name):\n",
    "  \n",
    "  df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "  num_tables = df_tables.count()\n",
    "  \n",
    "  return num_tables\n",
    "\n",
    "for listing in r_json:\n",
    "  try:\n",
    "    listing['summary']['share']['num_tables'] = get_num_tables_in_share(listing['summary']['share']['name'])\n",
    "  except:\n",
    "    print(f\"No share name: {listing['summary']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dfd14a7-de74-4d67-a6fd-93b75f43682e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebook Options\n",
    "This notebook contains four kinds of notebooks that can be generated, depending on how many tables are in the share, how large they are, etc.\n",
    "- **Pandas Profiling [Full]** - Applies full Pandas Profiling. Good for most shares.\n",
    "- **Pandas Profiling [Minimal]** - Applies a smaller version of Pandas Profiling. Good for shares with many tables, or large datasets with many columns/rows.\n",
    "- **Time-Series** - Gives a time-series plot for each numeric column, along with a scatter-matrix in some cases. Great for time-series datasets.\n",
    "- **Simple** - Only provides sample data and summary statistics for each table. Good for shares with very many tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bdcdaaf-e29e-44aa-a3a7-e8d51cec237c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pandas_profiling_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "def get_table_report(table_name):\n",
    "\n",
    "  from ydata_profiling import ProfileReport\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  displayHTML(ProfileReport(df).html)\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec66792b-ad69-47c5-a93c-fa4da7c0c7f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pandas_minimal_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "def get_table_report(table_name):\n",
    "\n",
    "  from ydata_profiling import ProfileReport\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  df = df.iloc[:,:10].sample(min(len(df),10000))\n",
    "  displayHTML(ProfileReport(df,minimal=True).html)\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbabd6e-3d26-4609-92bc-e4d5291ca650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "def get_time_series_plots(df,date_columns,num_columns):\n",
    "  if len(date_columns) >= 1:\n",
    "    date_column = date_columns[0]\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    for col in num_columns:\n",
    "        if df[col].isna().mean() < 0.5:\n",
    "          fig = px.area(df, x=date_column, y=col)\n",
    "          fig.update_layout(title='Time Series Plot for ' + col, xaxis_title=date_column, yaxis_title=col, \n",
    "                            font=dict(size=12))\n",
    "          fig.update_layout(yaxis_range=[0,1.5*df[col].max()])\n",
    "          fig.show()\n",
    "  else:\n",
    "    print('No datetime column found')\n",
    "      \n",
    "def get_scatter_matrix(df,num_columns):\n",
    "  if len(num_columns) > 1:\n",
    "    sm = px.scatter_matrix(df[num_columns])\n",
    "    sm.update_layout(title='Scatter Matrix Plot',\n",
    "                        font=dict(size=12))\n",
    "    sm.show()\n",
    "\n",
    "def get_table_report(table_name):\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  try:\n",
    "    print('Sample Data')\n",
    "    display(df.head(5))\n",
    "  except:\n",
    "    print('Sample Data cannot be displayed')  \n",
    "  date_columns = df.filter(regex=re.compile(r'.*(date|year|day|week|time).*', re.IGNORECASE)).columns\n",
    "  num_columns = [col for col in df.columns if col not in date_columns]\n",
    "  for col in num_columns:\n",
    "    df[col] = pd.to_numeric(df[col],errors='coerce')   \n",
    "  get_scatter_matrix(df,num_columns)\n",
    "  get_time_series_plots(df,date_columns,num_columns)\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d17937c-311e-46c8-984c-18a32a12a542",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def simple_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "def get_table_report(table_name):\n",
    "\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  \n",
    "  try:\n",
    "    print('Sample Data')\n",
    "    display(df.head(5))\n",
    "  except:\n",
    "    print('Sample Data cannot be displayed')\n",
    "  \n",
    "  try:\n",
    "    print('\\\\n\\\\nSummary Data')\n",
    "    display(df.describe().reset_index())\n",
    "  except:\n",
    "    print('Summary Data cannot be displayed')\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad1fdba-8fce-4ee5-8a4e-8c63a1546eba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_run_notebook(listing,notebook_type):\n",
    "  \"\"\"\n",
    "  Generate and run a new Databricks notebook based on the specified Marketplace listing and notebook type.\n",
    "\n",
    "  Args:\n",
    "  - listing: A JSON object representing a Marketplace listing.\n",
    "  - notebook_type: A string indicating the type of notebook to generate. Supported values are:\n",
    "      - 'Pandas Profiling [Full]'\n",
    "      - 'Pandas Profiling [Minimal]'\n",
    "      - 'Time-Series'\n",
    "      - 'Simple'\n",
    "\n",
    "  Returns:\n",
    "  - The notebook_run_id of the generated notebook to be attached to the listing.\n",
    "  \"\"\"\n",
    "    \n",
    "  l = listing\n",
    "\n",
    "  # GET ALL THE TABLES ASSOCIATED TO THIS PARTICULAR LISTING/SHARE\n",
    "  listing_id = l['id']\n",
    "  listing_name = l['summary']['name']\n",
    "  listing_subtitle = l['summary']['subtitle']\n",
    "  if 'share' not in l['summary']:\n",
    "    print(f\"Skipping - No Share: {listing_name}\")\n",
    "    return\n",
    "  share_name = l['summary']['share']['name']\n",
    "  status = l['summary']['status']\n",
    "  print(f\"Working on id={listing_id} name={listing_name}\")\n",
    "\n",
    "  df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "  t_names = [r.shared_object for r in df_tables.select(\"shared_object\").collect()]\n",
    "\n",
    "  notebook_name = f'{listing_id}_{share_name}'\n",
    "  new_path = os.path.join(os.path.dirname(notebook_path), notebook_name)\n",
    "\n",
    "  if notebook_type == 'Pandas Profiling [Full]':\n",
    "    content = pandas_profiling_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "  if notebook_type == 'Pandas Profiling [Minimal]':\n",
    "    content = pandas_minimal_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "  if notebook_type == 'Time-Series':\n",
    "    content = time_series_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "  elif notebook_type == 'Simple':\n",
    "    content = simple_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "\n",
    "  data = {\n",
    "    \"content\": base64.b64encode(content.encode(\"utf-8\")).decode('ascii'),\n",
    "    \"path\": new_path,\n",
    "    \"language\": \"PYTHON\",\n",
    "    \"overwrite\": True,\n",
    "    \"format\": \"SOURCE\"\n",
    "  }\n",
    "\n",
    "  response_created_notebook = requests.post(\n",
    "      f'{host_name}/api/2.0/workspace/import',\n",
    "      headers={'Authorization': f'Bearer {host_token}'},\n",
    "      json = data\n",
    "    ).json()\n",
    "\n",
    "  print(f\"{response_created_notebook} - Created for {new_path}.. Running {notebook_name}\")\n",
    "\n",
    "  ex = dbutils.notebook.run(notebook_name,timeout_seconds=600)\n",
    "  return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07aa134-8809-4760-b75b-0c1328c1a44a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_notebook_to_marketplace(listing_id,notebook_run_id,handle_existing_notebook='replace'):\n",
    "  \"\"\"\n",
    "  Uploads a notebook to a Databricks Marketplace listing.\n",
    "\n",
    "  Args:\n",
    "  - listing_id: the ID of the listing to upload the notebook to\n",
    "  - notebook_run_id: the ID of the notebook run that generated the HTML content to upload (from generate_and_run_notebook)\n",
    "  - handle_existing_notebook: what to do if the listing already has a notebook attached to it\n",
    "      - 'replace': replace the existing notebook with the new one (default)\n",
    "      - 'skip': skip the upload and do nothing\n",
    "      - 'append': add as a second notebook\n",
    "\n",
    "  Returns: None. Attaches given notebook to given listing.\n",
    "  \"\"\"\n",
    "  individual_listing_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/marketplace-provider/listings/{listing_id}\"\n",
    "  get_listing_r = requests.get(individual_listing_url, headers={'Authorization': f'Bearer {host_token}'})\n",
    "  payload = get_listing_r.json()\n",
    "  cur_files = payload[\"listing\"][\"detail\"].get(\"embedded_notebook_file_infos\", [])\n",
    "  \n",
    "  if len(cur_files) > 0:\n",
    "    if handle_existing_notebook == 'skip':\n",
    "      print(f\"Skipping {listing_id} bc it has a notebook already attached to it\")\n",
    "      return\n",
    "  \n",
    "  run_id = notebook_run_id\n",
    "  run_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/jobs/runs/export?run_id={run_id}\"\n",
    "  run_response = requests.get(run_url, headers={'Authorization': f'Bearer {host_token}'})\n",
    "  html_content = run_response.json()['views'][0]['content']\n",
    "  print(f\"Successfully extracted HTML content for run_id={run_id} and listing_id={listing_id}\")\n",
    "  \n",
    "  file_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/marketplace-provider/files\"\n",
    "  file_body = {\n",
    "      \"marketplace_file_type\": \"EMBEDDED_NOTEBOOK\",\n",
    "      \"file_parent\": {\n",
    "          \"parent_id\": listing_id,\n",
    "          \"file_parent_type\": \"LISTING\"\n",
    "      },\n",
    "      \"mime_type\": \"text/html\"\n",
    "  }\n",
    "  file_r  = requests.post(file_url, headers={'Authorization': f'Bearer {host_token}'}, json=file_body)\n",
    "  file_r_json = file_r.json()\n",
    "  file_upload_url = file_r_json['upload_url']\n",
    "  file_info_id = file_r_json['file_info']['id']\n",
    "  print(f\"Successfully created a file for listing_id={listing_id} file_info_id={file_info_id}\")\n",
    "  \n",
    "  # given the pre-signed URL, upload the HTML content to it\n",
    "  with open(f\"{listing_id}.html\", 'w') as fa:\n",
    "    fa.write(html_content)\n",
    "    r_put_html = requests.put(file_upload_url, data=open(f\"{listing_id}.html\",'rb').read(), headers={\"x-amz-server-side-encryption\": \"AES256\", 'Content-Type': 'text/html'})\n",
    "    print(f\"Received {r_put_html} from PUT to S3 pre-signed URL\")\n",
    "    if int(r_put_html.status_code) != 200:\n",
    "      print(r_put_html.text)\n",
    "      \n",
    "  # Final step: Handle the response of the PUT, so we can attach the file to the Listing\n",
    "  if handle_existing_notebook == 'append':\n",
    "    cur_files.append({\"id\": file_info_id})\n",
    "  else:\n",
    "    cur_files = [{\"id\": file_info_id}]\n",
    "  payload[\"listing\"][\"detail\"][\"embedded_notebook_file_infos\"] = cur_files\n",
    "  \n",
    "  payload[\"listing\"][\"summary\"][\"provider_info\"][\"name\"] = provider_name\n",
    "  payload[\"listing\"][\"deployment_name\"] = deployment_name\n",
    "  payload['listing']['summary']['provider_info']['business_contact_email'] = business_contact_email\n",
    "  payload['listing']['summary']['provider_info']['term_of_service_link'] = terms_of_service_link\n",
    "  payload['listing']['summary']['provider_info']['privacy_policy_link'] = privacy_policy_link\n",
    "  new_l = len(payload[\"listing\"][\"detail\"][\"embedded_notebook_file_infos\"])\n",
    "  print(f\"New length of embedded_notebook_file_infos -> {new_l}\")\n",
    "  j_payload = json.dumps(payload)\n",
    "  r = requests.put(individual_listing_url, data=j_payload, headers={'Authorization': f'Bearer {host_token}'})\n",
    "  print(f\"Received {r} from PUT to update listing_id={listing_id}\")\n",
    "  print('-------------------------------------------')\n",
    "  if int(r.status_code) != 200:\n",
    "      print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19166acb-d24e-4300-b88c-d81b827357a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_notebook_for_listing(listing,notebook_type,handle_existing_notebook='replace'):\n",
    "  \"\"\"\n",
    "  Generates, runs, and attaches a notebook to a listing.\n",
    "\n",
    "  Args:\n",
    "  - listing: A JSON object representing a Marketplace listing.\n",
    "  - notebook_type: A string indicating the type of notebook to generate. Supported values are:\n",
    "      - 'Pandas Profiling [Full]'\n",
    "      - 'Pandas Profiling [Minimal]'\n",
    "      - 'Time-Series'\n",
    "      - 'Simple'\n",
    "  - handle_existing_notebook: what to do if the listing already has a notebook attached to it\n",
    "      - 'replace': replace the existing notebook with the new one (default)\n",
    "      - 'skip': skip the upload and do nothing\n",
    "      - 'append': add as an additional notebook\n",
    "\n",
    "  Returns:\n",
    "  - None. Generates, runs, and attaches notebook to DBM listing.\n",
    "  \"\"\"\n",
    "  \n",
    "  listing_id = listing['id']\n",
    "  \n",
    "  try:\n",
    "    notebook_run_id = generate_and_run_notebook(listing, notebook_type)\n",
    "  except:\n",
    "    print(f\"Notebook run failed for listing_id={listing_id}\")\n",
    "  \n",
    "  try:\n",
    "    \n",
    "    upload_notebook_to_marketplace(listing_id,notebook_run_id)\n",
    "  except:\n",
    "    print(f\"Notebook upload failed for listing_id={listing_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5726157-cf9b-4f2a-b997-252eccac8b15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create and Attach Notebooks\n",
    "######To create, run, and attach notebooks to listings:\n",
    "- run 'create_notebook_for_listing' for each listing in r_json\n",
    "- Decide for each listing what method you would like to use \n",
    "  - 'Pandas Profiling [Full]' \n",
    "  - 'Pandas Profiling [Minimal]'\n",
    "  - 'Time-Series'\n",
    "  - 'Simple'\n",
    "- Decide for each listing how you would like to handle listings with existing notebooks\n",
    "  - 'replace'\n",
    "  - 'skip'\n",
    "  - 'append'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbb1fa4-4dc3-4fb3-b3c0-554898a664c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_notebook_for_listing(listing,notebook_type,handle_existing_notebook)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2870152860832532,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Notebook Generation (Shareable)",
   "notebookOrigID": 3716794575589387,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
