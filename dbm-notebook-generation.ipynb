{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9997b921-ecdc-45d2-b82e-f73cd3281893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4176ac-8bcd-4b0f-a7a3-2489af1403b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Package Imports\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039d6bf0-4720-440a-9762-7c0a01b5620d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enter Databricks account hash here:\n",
    "db_account_hash = ''\n",
    "\n",
    "# Enter Databricks Provider Name here:\n",
    "provider_name = ''\n",
    "\n",
    "# Enter Contact Email here:\n",
    "business_contact_email = ''\n",
    "\n",
    "# Enter TOS Link here:\n",
    "terms_of_service_link = ''\n",
    "\n",
    "# Enter Privacy Policy Link here:\n",
    "privacy_policy_link = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8870ae2a-fd54-4b27-bce6-82cf86fc4b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the API URL associated with the notebook's context, if it exists\n",
    "host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "\n",
    "# Retrieve the API token associated with the notebook's context, if it exists\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "# Retrieve the user associated with the notebook's context, if it exists\n",
    "user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").getOrElse(None)\n",
    "\n",
    "# Retrieve the workspace ID associated with the notebook's context, if it exists\n",
    "workspace_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"orgId\").getOrElse(None)\n",
    "\n",
    "# Convert the notebook's context to a dictionary\n",
    "ctx = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())\n",
    "\n",
    "# Extract the API URL from the 'extraContext' key of the notebook's context dictionary\n",
    "host_name = ctx['extraContext']['api_url']\n",
    "\n",
    "# Extract the API token from the 'extraContext' key of the notebook's context dictionary\n",
    "host_token = ctx['extraContext']['api_token']\n",
    "\n",
    "# Extract the notebook path from the 'extraContext' key of the notebook's context dictionary\n",
    "notebook_path = ctx['extraContext']['notebook_path']\n",
    "\n",
    "# Set Deployment Name:\n",
    "deployment_name = db_account_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cda969-1f09-4385-817d-13dd5e6d7cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Get all Marketplace listings\n",
    "get_listings_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/marketplace-provider/listings\"\n",
    "response = requests.get(\n",
    "          get_listings_url,\n",
    "          headers={\"Authorization\": \"Bearer \" + token})\n",
    "\n",
    "r_json = response.json()\n",
    "r_json = [listing for listing in r_json['listings']]\n",
    "\n",
    "print(f\"API returned {len(response.json()['listings'])} listing(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64314ee-4e0c-4dc7-b94a-b110f9a3084f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## For each share, find number of tables in share and add this number to this listing metadata to determine what type of Pandas Profiling to use\n",
    "def get_num_tables_in_share(share_name):\n",
    "  \n",
    "  df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "  num_tables = df_tables.count()\n",
    "  \n",
    "  return num_tables\n",
    "\n",
    "for listing in r_json:\n",
    "  try:\n",
    "    listing['summary']['share']['num_tables'] = get_num_tables_in_share(listing['summary']['share']['name'])\n",
    "  except:\n",
    "    print(f\"No share name: {listing['summary']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dfd14a7-de74-4d67-a6fd-93b75f43682e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebook Options\n",
    "This notebook contains four kinds of notebooks that can be generated, depending on how many tables are in the share, how large they are, etc.\n",
    "- **Pandas Profiling [Full]** - Applies full Pandas Profiling. Good for most shares.\n",
    "- **Pandas Profiling [Minimal]** - Applies a smaller version of Pandas Profiling. Good for shares with many tables, or large datasets with many columns/rows.\n",
    "- **Time-Series** - Gives a time-series plot for each numeric column, along with a scatter-matrix in some cases. Great for time-series datasets.\n",
    "- **Simple** - Only provides sample data and summary statistics for each table. Good for shares with very many tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bdcdaaf-e29e-44aa-a3a7-e8d51cec237c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pandas_profiling_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "def get_table_report(table_name):\n",
    "\n",
    "  from ydata_profiling import ProfileReport\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  displayHTML(ProfileReport(df).html)\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec66792b-ad69-47c5-a93c-fa4da7c0c7f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pandas_minimal_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "def get_table_report(table_name):\n",
    "\n",
    "  from ydata_profiling import ProfileReport\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  df = df.iloc[:,:10].sample(min(len(df),10000))\n",
    "  displayHTML(ProfileReport(df,minimal=True).html)\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbabd6e-3d26-4609-92bc-e4d5291ca650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "def get_time_series_plots(df,date_columns,num_columns):\n",
    "  if len(date_columns) >= 1:\n",
    "    date_column = date_columns[0]\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    for col in num_columns:\n",
    "        if df[col].isna().mean() < 0.5:\n",
    "          fig = px.area(df, x=date_column, y=col)\n",
    "          fig.update_layout(title='Time Series Plot for ' + col, xaxis_title=date_column, yaxis_title=col, \n",
    "                            font=dict(size=12))\n",
    "          fig.update_layout(yaxis_range=[0,1.5*df[col].max()])\n",
    "          fig.show()\n",
    "  else:\n",
    "    print('No datetime column found')\n",
    "      \n",
    "def get_scatter_matrix(df,num_columns):\n",
    "  if len(num_columns) > 1:\n",
    "    sm = px.scatter_matrix(df[num_columns])\n",
    "    sm.update_layout(title='Scatter Matrix Plot',\n",
    "                        font=dict(size=12))\n",
    "    sm.show()\n",
    "\n",
    "def get_table_report(table_name):\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  try:\n",
    "    print('Sample Data')\n",
    "    display(df.head(5))\n",
    "  except:\n",
    "    print('Sample Data cannot be displayed')  \n",
    "  date_columns = df.filter(regex=re.compile(r'.*(date|year|day|week|time).*', re.IGNORECASE)).columns\n",
    "  num_columns = [col for col in df.columns if col not in date_columns]\n",
    "  for col in num_columns:\n",
    "    df[col] = pd.to_numeric(df[col],errors='coerce')   \n",
    "  get_scatter_matrix(df,num_columns)\n",
    "  get_time_series_plots(df,date_columns,num_columns)\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d17937c-311e-46c8-984c-18a32a12a542",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def simple_notebook(listing_name, listing_subtitle, share_name, table_names=[]):\n",
    "  \n",
    "  def get_table_commands(table_name):\n",
    "    table_model = f\"\"\"  \n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Name: {table_name}\n",
    "\n",
    "get_table_report(\"{table_name}\")\"\"\"\n",
    "    return table_model\n",
    "  table_block = \"\"\n",
    "  \n",
    "  for table_name in table_names:\n",
    "    table_block += get_table_commands(table_name)\n",
    "    \n",
    "  model_notebook = f\"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # {listing_name}\n",
    "# MAGIC {listing_subtitle}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Tables Available in this Share\n",
    "share_name = \"{share_name}\"\n",
    "df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "print(f\"This share contains {len(table_names)} table(s)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Show all Tables available in this Share\n",
    "display(df_tables.select(\"name\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Exploration\n",
    "\n",
    "# COMMAND\n",
    "def get_table_report(table_name):\n",
    "\n",
    "  df = sqlContext.sql(f\"select * from {{table_name}}\").toPandas()\n",
    "  \n",
    "  try:\n",
    "    print('Sample Data')\n",
    "    display(df.head(5))\n",
    "  except:\n",
    "    print('Sample Data cannot be displayed')\n",
    "  \n",
    "  try:\n",
    "    print('\\\\n\\\\nSummary Data')\n",
    "    display(df.describe().reset_index())\n",
    "  except:\n",
    "    print('Summary Data cannot be displayed')\n",
    "\n",
    "{table_block}\n",
    "\n",
    "# COMMAND ----------\n",
    "# Ignore if notebook is not being run as a job\n",
    "import json\n",
    "dbutils.notebook.exit(json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())['currentRunId']['id'])\"\"\"\n",
    "  return model_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad1fdba-8fce-4ee5-8a4e-8c63a1546eba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_run_notebook(listing,notebook_type):\n",
    "  \"\"\"\n",
    "  Generate and run a new Databricks notebook based on the specified Marketplace listing and notebook type.\n",
    "\n",
    "  Args:\n",
    "  - listing: A JSON object representing a Marketplace listing.\n",
    "  - notebook_type: A string indicating the type of notebook to generate. Supported values are:\n",
    "      - 'Pandas Profiling [Full]'\n",
    "      - 'Pandas Profiling [Minimal]'\n",
    "      - 'Time-Series'\n",
    "      - 'Simple'\n",
    "\n",
    "  Returns:\n",
    "  - The notebook_run_id of the generated notebook to be attached to the listing.\n",
    "  \"\"\"\n",
    "    \n",
    "  l = listing\n",
    "\n",
    "  # GET ALL THE TABLES ASSOCIATED TO THIS PARTICULAR LISTING/SHARE\n",
    "  listing_id = l['id']\n",
    "  listing_name = l['summary']['name']\n",
    "  listing_subtitle = l['summary']['subtitle']\n",
    "  if 'share' not in l['summary']:\n",
    "    print(f\"Skipping - No Share: {listing_name}\")\n",
    "    return\n",
    "  share_name = l['summary']['share']['name']\n",
    "  status = l['summary']['status']\n",
    "  print(f\"Working on id={listing_id} name={listing_name}\")\n",
    "\n",
    "  df_tables = spark.sql(f\"SHOW ALL IN SHARE {share_name}\")\n",
    "  t_names = [r.shared_object for r in df_tables.select(\"shared_object\").collect()]\n",
    "\n",
    "  notebook_name = f'{listing_id}_{share_name}'\n",
    "  new_path = os.path.join(os.path.dirname(notebook_path), notebook_name)\n",
    "\n",
    "  if notebook_type == 'Pandas Profiling [Full]':\n",
    "    content = pandas_profiling_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "  if notebook_type == 'Pandas Profiling [Minimal]':\n",
    "    content = pandas_minimal_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "  if notebook_type == 'Time-Series':\n",
    "    content = time_series_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "  elif notebook_type == 'Simple':\n",
    "    content = simple_notebook(listing_name, listing_subtitle, share_name, t_names)\n",
    "\n",
    "  data = {\n",
    "    \"content\": base64.b64encode(content.encode(\"utf-8\")).decode('ascii'),\n",
    "    \"path\": new_path,\n",
    "    \"language\": \"PYTHON\",\n",
    "    \"overwrite\": True,\n",
    "    \"format\": \"SOURCE\"\n",
    "  }\n",
    "\n",
    "  response_created_notebook = requests.post(\n",
    "      f'{host_name}/api/2.0/workspace/import',\n",
    "      headers={'Authorization': f'Bearer {host_token}'},\n",
    "      json = data\n",
    "    ).json()\n",
    "\n",
    "  print(f\"{response_created_notebook} - Created for {new_path}.. Running {notebook_name}\")\n",
    "\n",
    "  ex = dbutils.notebook.run(notebook_name,timeout_seconds=600)\n",
    "  return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07aa134-8809-4760-b75b-0c1328c1a44a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_notebook_to_marketplace(listing_id,notebook_run_id,handle_existing_notebook='replace'):\n",
    "  \"\"\"\n",
    "  Uploads a notebook to a Databricks Marketplace listing.\n",
    "\n",
    "  Args:\n",
    "  - listing_id: the ID of the listing to upload the notebook to\n",
    "  - notebook_run_id: the ID of the notebook run that generated the HTML content to upload (from generate_and_run_notebook)\n",
    "  - handle_existing_notebook: what to do if the listing already has a notebook attached to it\n",
    "      - 'replace': replace the existing notebook with the new one (default)\n",
    "      - 'skip': skip the upload and do nothing\n",
    "      - 'append': add as a second notebook\n",
    "\n",
    "  Returns: None. Attaches given notebook to given listing.\n",
    "  \"\"\"\n",
    "  individual_listing_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/marketplace-provider/listings/{listing_id}\"\n",
    "  get_listing_r = requests.get(individual_listing_url, headers={'Authorization': f'Bearer {host_token}'})\n",
    "  payload = get_listing_r.json()\n",
    "  cur_files = payload[\"listing\"][\"detail\"].get(\"embedded_notebook_file_infos\", [])\n",
    "  \n",
    "  if len(cur_files) > 0:\n",
    "    if handle_existing_notebook == 'skip':\n",
    "      print(f\"Skipping {listing_id} bc it has a notebook already attached to it\")\n",
    "      return\n",
    "  \n",
    "  run_id = notebook_run_id\n",
    "  run_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/jobs/runs/export?run_id={run_id}\"\n",
    "  run_response = requests.get(run_url, headers={'Authorization': f'Bearer {host_token}'})\n",
    "  html_content = run_response.json()['views'][0]['content']\n",
    "  print(f\"Successfully extracted HTML content for run_id={run_id} and listing_id={listing_id}\")\n",
    "  \n",
    "  file_url = f\"https://{db_account_hash}.cloud.databricks.com/api/2.0/marketplace-provider/files\"\n",
    "  file_body = {\n",
    "      \"marketplace_file_type\": \"EMBEDDED_NOTEBOOK\",\n",
    "      \"file_parent\": {\n",
    "          \"parent_id\": listing_id,\n",
    "          \"file_parent_type\": \"LISTING\"\n",
    "      },\n",
    "      \"mime_type\": \"text/html\"\n",
    "  }\n",
    "  file_r  = requests.post(file_url, headers={'Authorization': f'Bearer {host_token}'}, json=file_body)\n",
    "  file_r_json = file_r.json()\n",
    "  file_upload_url = file_r_json['upload_url']\n",
    "  file_info_id = file_r_json['file_info']['id']\n",
    "  print(f\"Successfully created a file for listing_id={listing_id} file_info_id={file_info_id}\")\n",
    "  \n",
    "  # given the pre-signed URL, upload the HTML content to it\n",
    "  with open(f\"{listing_id}.html\", 'w') as fa:\n",
    "    fa.write(html_content)\n",
    "    r_put_html = requests.put(file_upload_url, data=open(f\"{listing_id}.html\",'rb').read(), headers={\"x-amz-server-side-encryption\": \"AES256\", 'Content-Type': 'text/html'})\n",
    "    print(f\"Received {r_put_html} from PUT to S3 pre-signed URL\")\n",
    "    if int(r_put_html.status_code) != 200:\n",
    "      print(r_put_html.text)\n",
    "      \n",
    "  # Final step: Handle the response of the PUT, so we can attach the file to the Listing\n",
    "  if handle_existing_notebook == 'append':\n",
    "    cur_files.append({\"id\": file_info_id})\n",
    "  else:\n",
    "    cur_files = [{\"id\": file_info_id}]\n",
    "  payload[\"listing\"][\"detail\"][\"embedded_notebook_file_infos\"] = cur_files\n",
    "  \n",
    "  payload[\"listing\"][\"summary\"][\"provider_info\"][\"name\"] = provider_name\n",
    "  payload[\"listing\"][\"deployment_name\"] = deployment_name\n",
    "  payload['listing']['summary']['provider_info']['business_contact_email'] = business_contact_email\n",
    "  payload['listing']['summary']['provider_info']['term_of_service_link'] = terms_of_service_link\n",
    "  payload['listing']['summary']['provider_info']['privacy_policy_link'] = privacy_policy_link\n",
    "  new_l = len(payload[\"listing\"][\"detail\"][\"embedded_notebook_file_infos\"])\n",
    "  print(f\"New length of embedded_notebook_file_infos -> {new_l}\")\n",
    "  j_payload = json.dumps(payload)\n",
    "  r = requests.put(individual_listing_url, data=j_payload, headers={'Authorization': f'Bearer {host_token}'})\n",
    "  print(f\"Received {r} from PUT to update listing_id={listing_id}\")\n",
    "  print('-------------------------------------------')\n",
    "  if int(r.status_code) != 200:\n",
    "      print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19166acb-d24e-4300-b88c-d81b827357a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_notebook_for_listing(listing,notebook_type,handle_existing_notebook='replace'):\n",
    "  \"\"\"\n",
    "  Generates, runs, and attaches a notebook to a listing.\n",
    "\n",
    "  Args:\n",
    "  - listing: A JSON object representing a Marketplace listing.\n",
    "  - notebook_type: A string indicating the type of notebook to generate. Supported values are:\n",
    "      - 'Pandas Profiling [Full]'\n",
    "      - 'Pandas Profiling [Minimal]'\n",
    "      - 'Time-Series'\n",
    "      - 'Simple'\n",
    "  - handle_existing_notebook: what to do if the listing already has a notebook attached to it\n",
    "      - 'replace': replace the existing notebook with the new one (default)\n",
    "      - 'skip': skip the upload and do nothing\n",
    "      - 'append': add as an additional notebook\n",
    "\n",
    "  Returns:\n",
    "  - None. Generates, runs, and attaches notebook to DBM listing.\n",
    "  \"\"\"\n",
    "  \n",
    "  listing_id = listing['id']\n",
    "  \n",
    "  try:\n",
    "    notebook_run_id = generate_and_run_notebook(listing, notebook_type)\n",
    "  except:\n",
    "    print(f\"Notebook run failed for listing_id={listing_id}\")\n",
    "  \n",
    "  try:\n",
    "    \n",
    "    upload_notebook_to_marketplace(listing_id,notebook_run_id)\n",
    "  except:\n",
    "    print(f\"Notebook upload failed for listing_id={listing_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5726157-cf9b-4f2a-b997-252eccac8b15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create and Attach Notebooks\n",
    "######To create, run, and attach notebooks to listings:\n",
    "- run 'create_notebook_for_listing' for each listing in r_json\n",
    "- Decide for each listing what method you would like to use \n",
    "  - 'Pandas Profiling [Full]' \n",
    "  - 'Pandas Profiling [Minimal]'\n",
    "  - 'Time-Series'\n",
    "  - 'Simple'\n",
    "- Decide for each listing how you would like to handle listings with existing notebooks\n",
    "  - 'replace'\n",
    "  - 'skip'\n",
    "  - 'append'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbb1fa4-4dc3-4fb3-b3c0-554898a664c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_notebook_for_listing(listing,notebook_type,handle_existing_notebook)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2870152860832532,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Notebook Generation (Shareable)",
   "notebookOrigID": 3716794575589387,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
